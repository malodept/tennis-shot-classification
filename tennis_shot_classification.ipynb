{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y numpy torch torchvision torchaudio\n",
        "!pip -q install numpy==1.26.4\n",
        "!pip -q install torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install opencv-python-headless==4.10.0.84 transformers==4.44.2 timm==1.0.9\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oT63oZFlUnH",
        "outputId": "fb9a135a-faf0-4bd0-c342-b60c1e94853c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.4 requires torch<2.9,>=1.10, which is not installed.\n",
            "fastai 2.8.4 requires torchvision>=0.11, which is not installed.\n",
            "sentence-transformers 5.1.0 requires torch>=1.11.0, which is not installed.\n",
            "peft 0.17.1 requires torch>=1.13.0, which is not installed.\n",
            "accelerate 1.10.1 requires torch>=2.0.0, which is not installed.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Redémarrer le runtime entre ces deux cellules"
      ],
      "metadata": {
        "id": "GCCxn7K8zRyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy, torch, torchvision, cv2\n",
        "print(\"numpy\", numpy.__version__)\n",
        "print(\"torch\", torch.__version__)\n",
        "print(\"torchvision\", torchvision.__version__)\n",
        "print(\"cv2\", cv2.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBQOYX9zjGCo",
        "outputId": "7576db7e-35ea-47e1-e5f8-7154d8a1e110"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy 1.26.4\n",
            "torch 2.4.1+cu121\n",
            "torchvision 0.19.1+cu121\n",
            "cv2 4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE = \"/content/drive/MyDrive\"\n",
        "NPZ_DIR = f\"{BASE}/processed_npz\"\n",
        "!ls -lah \"{NPZ_DIR}\" | head -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0uRpQ7C4BL3",
        "outputId": "ced589c7-9393-4789-f9cd-66173c44a7eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "total 191M\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0001.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0002.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0003.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0004.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0005.npz\n",
            "-rw------- 1 root root 1.5M Sep 16 23:00 backhand_swing_0006.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0007.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0008.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0009.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0010.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0011.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0012.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0013.npz\n",
            "-rw------- 1 root root 1.3M Sep 16 23:00 backhand_swing_0014.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0015.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0016.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0017.npz\n",
            "-rw------- 1 root root 1.4M Sep 16 23:00 backhand_swing_0018.npz\n",
            "-rw------- 1 root root 1.3M Sep 16 23:01 backhand_swing_0019.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset_colab.py\n",
        "import numpy as np, torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "TCLIP = 8\n",
        "\n",
        "class TennisDataset(Dataset):\n",
        "    def __init__(self, files, train=True):\n",
        "        self.files = [str(p) for p in files]\n",
        "        self.train = train\n",
        "        self.train_tf = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.RandomResizedCrop(192, scale=(0.8, 1.0)),\n",
        "            T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "            T.RandomGrayscale(p=0.1),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485, 0.456, 0.406],\n",
        "                        [0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        self.eval_tf = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.Resize((192, 192)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485, 0.456, 0.406],\n",
        "                        [0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        d = np.load(self.files[idx])\n",
        "        X, y = d[\"X\"], int(d[\"y\"])      # X: [T,H,W,C]\n",
        "        # échantillonnage à 8 frames\n",
        "        idxs = np.linspace(0, X.shape[0]-1, TCLIP).astype(int)\n",
        "        tf = self.train_tf if self.train else self.eval_tf\n",
        "        frames = [tf(X[i]) for i in idxs]\n",
        "        video = torch.stack(frames, dim=0)   # [T=8,C,H,W]\n",
        "        return video, y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyX9VMCb4kMt",
        "outputId": "e6e0d198-db73-4209-81e6-371b28b55fbe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dataset_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_colab.py\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pathlib import Path\n",
        "from transformers import TimesformerForVideoClassification\n",
        "from dataset_colab import TennisDataset\n",
        "\n",
        "BASE = \"/content/drive/MyDrive\"\n",
        "NPZ_DIR = Path(f\"{BASE}/processed_npz\")\n",
        "\n",
        "files = sorted(NPZ_DIR.glob(\"*.npz\"))\n",
        "assert len(files) > 0, \"Aucun .npz trouvé dans processed_npz\"\n",
        "\n",
        "# split 70/15/15\n",
        "n = len(files); n_tr = int(0.7*n); n_val = int(0.15*n); n_te = n - n_tr - n_val\n",
        "tr, val, te = random_split(files, [n_tr, n_val, n_te], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_ds = TennisDataset(tr, train=True)\n",
        "val_ds   = TennisDataset(val, train=False)\n",
        "test_ds  = TennisDataset(te, train=False)\n",
        "\n",
        "BATCH, ACCUM = 4, 4\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = TimesformerForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    num_labels=2, ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# 8 frames, 192px\n",
        "model.config.num_frames = 8\n",
        "model.config.image_size = 192\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "EPOCHS_LP, EPOCHS_FT = 5, 25\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_LP+EPOCHS_FT, eta_min=1e-5)\n",
        "scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "# linear probe 5 ep\n",
        "for p in model.parameters(): p.requires_grad = False\n",
        "for p in model.classifier.parameters(): p.requires_grad = True\n",
        "\n",
        "def train_one_epoch(loader):\n",
        "    model.train(); optimizer.zero_grad(set_to_none=True)\n",
        "    for step, (X, y) in enumerate(loader, 1):\n",
        "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "            out = model(X)                          # X: [B,8,C,192,192]\n",
        "            loss = criterion(out.logits, y) / ACCUM\n",
        "        scaler.scale(loss).backward()\n",
        "        if step % ACCUM == 0:\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval(); correct=total=0\n",
        "    with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X).logits.argmax(1)\n",
        "            correct += (pred==y).sum().item(); total += y.size(0)\n",
        "    return 100*correct/total\n",
        "\n",
        "\n",
        "best = 0.0; best_path = str(Path(BASE) / \"best_timesformer.pt\")\n",
        "\n",
        "for epoch in range(EPOCHS_LP):\n",
        "    train_one_epoch(train_loader)\n",
        "    acc = evaluate(val_loader)\n",
        "    print(f\"[LP] Epoch {epoch+1}/{EPOCHS_LP} Val acc {acc:.2f}\")\n",
        "    if acc > best: best = acc; torch.save(model.state_dict(), best_path)\n",
        "    scheduler.step()\n",
        "\n",
        "for p in model.parameters(): p.requires_grad = True\n",
        "\n",
        "for epoch in range(EPOCHS_FT):\n",
        "    train_one_epoch(train_loader)\n",
        "    acc = evaluate(val_loader)\n",
        "    print(f\"[FT] Epoch {epoch+1}/{EPOCHS_FT} Val acc {acc:.2f}\")\n",
        "    if acc > best: best = acc; torch.save(model.state_dict(), best_path)\n",
        "    scheduler.step()\n",
        "\n",
        "model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "test_acc = evaluate(test_loader)\n",
        "print(f\"Test acc {test_acc:.2f}  |  Best val {best:.2f}\")\n"
      ],
      "metadata": {
        "id": "oVQ4Ijnp4r6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193045e3-fec9-4865-b94d-5a9e1790ee23"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "gc.collect(); torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "pwFSw9Gx31bB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_colab.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1AgXXQw1DPy",
        "outputId": "4070fb06-0ed4-4679-e211-d5b0407650c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "[LP] Epoch 1/5 Val acc 61.11\n",
            "[LP] Epoch 2/5 Val acc 50.00\n",
            "[LP] Epoch 3/5 Val acc 55.56\n",
            "[LP] Epoch 4/5 Val acc 55.56\n",
            "[LP] Epoch 5/5 Val acc 72.22\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "[FT] Epoch 1/25 Val acc 55.56\n",
            "[FT] Epoch 2/25 Val acc 55.56\n",
            "[FT] Epoch 3/25 Val acc 44.44\n",
            "[FT] Epoch 4/25 Val acc 55.56\n",
            "[FT] Epoch 5/25 Val acc 66.67\n",
            "[FT] Epoch 6/25 Val acc 83.33\n",
            "[FT] Epoch 7/25 Val acc 66.67\n",
            "[FT] Epoch 8/25 Val acc 88.89\n",
            "[FT] Epoch 9/25 Val acc 66.67\n",
            "[FT] Epoch 10/25 Val acc 77.78\n",
            "[FT] Epoch 11/25 Val acc 77.78\n",
            "[FT] Epoch 12/25 Val acc 72.22\n",
            "[FT] Epoch 13/25 Val acc 77.78\n",
            "[FT] Epoch 14/25 Val acc 77.78\n",
            "[FT] Epoch 15/25 Val acc 72.22\n",
            "[FT] Epoch 16/25 Val acc 66.67\n",
            "[FT] Epoch 17/25 Val acc 77.78\n",
            "[FT] Epoch 18/25 Val acc 83.33\n",
            "[FT] Epoch 19/25 Val acc 83.33\n",
            "[FT] Epoch 20/25 Val acc 88.89\n",
            "[FT] Epoch 21/25 Val acc 88.89\n",
            "[FT] Epoch 22/25 Val acc 83.33\n",
            "[FT] Epoch 23/25 Val acc 83.33\n",
            "[FT] Epoch 24/25 Val acc 83.33\n",
            "[FT] Epoch 25/25 Val acc 83.33\n",
            "/content/train_colab.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_path, map_location=device))\n",
            "Test acc 94.74  |  Best val 88.89\n"
          ]
        }
      ]
    }
  ]
}